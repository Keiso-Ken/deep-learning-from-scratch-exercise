{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFyPpeWhcrbP"
   },
   "source": [
    "# 4章 word2vec の高速化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23cSHUC_cwsA"
   },
   "source": [
    "CBOW: Continuous Bag-of-Words\n",
    "- 推論ベースの手法\n",
    "- 前後の単語（コンテキスト）から，その単語（ターゲット）を推定するNN\n",
    "\n",
    "**you (　) goodbye and I say hello**\n",
    "\n",
    "- one-hot 表現：単語数を$N$とすると，$N$次元の0-1ベクトルで単語を表現\n",
    "- [図１](https://www.dropbox.com/s/v1hogfasdqmqssh/fig1.jpg?dl=0)\n",
    "- $N$が大きくなると，学習が大変"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOxroymEeynx"
   },
   "source": [
    "## 4.1 word2vecの改良(1)\n",
    "- **入力層のベクトルと重み行列$W_{\\rm in}$との積**\n",
    "- 中間層のベクトルと重み行列$W_{\\rm out}$との積\n",
    "- softmaxの計算\n",
    "\n",
    "### 4.1.1 embedding レイヤ\n",
    "- 入力層 $c = [0,0,1,0,0,\\ldots,0]$: 横ベクトル（$N$次元）\n",
    "- 重み行列 $W_{\\rm in}\\in\\mathbb{R}^{N\\times M}$\n",
    "$$\n",
    "  W_{\\rm in} = \\begin{bmatrix}W_{\\rm in}(1)\\\\W_{\\rm in}(2)\\\\\\vdots\\\\W_{\\rm in}(N)\\end{bmatrix}\n",
    "$$\n",
    "- $c$と$W_{\\rm in}$との積\n",
    "$$\n",
    " \\begin{split}\n",
    " cW_{\\rm in} &= 0\\cdot W_{\\rm in}(0) + 0\\cdot W_{\\rm in}(1) + 1 \\cdot W_{\\rm in}(2) + 0\\cdot W_{\\rm in}(3) + \\cdots + 0\\cdot W_{\\rm in}(N)\\\\\n",
    " &= W_{\\rm in}(2)\n",
    " \\end{split}\n",
    "$$\n",
    "- 一般に $i$番目の単語のone-hot表現と$W_{\\rm in}$との積は，$i$行目の行ベクトル$W_{\\rm in}(i)$ を抜き出す操作となる．\n",
    "- これを **Embedding レイヤ**（埋め込みレイヤ）と呼ぶ\n",
    " - Embedding レイヤ：重み行列から$i$行目を抜き出すレイヤ\n",
    "- Embedding（埋め込み）：単語（離散的なデータ）を$M$次元実数空間$\\mathbb{R}^M$に埋め込む\n",
    "\n",
    "### 4.1.2 embedding レイヤの実装\n",
    "- 順伝播と逆伝播\n",
    " - [図２](https://www.dropbox.com/s/mw59gd27wbuzwie/fig2.jpg?dl=0)\n",
    "- バッチ処理のとき\n",
    " - [図３](https://www.dropbox.com/s/wjne7npo6arwxxw/fig3.jpg?dl=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYz7oKVDlL7R"
   },
   "source": [
    "## 4.2 word2vecの改良(2)\n",
    "\n",
    "### 4.2.1 中間層以降の計算の問題点\n",
    "- 入力層のベクトルと重み行列$W_{\\rm in}$との積\n",
    "- **中間層のベクトルと重み行列$W_{\\rm out}$との積**\n",
    "- **softmaxの計算**\n",
    "\n",
    "### 4.2.2 多値分類から２値分類へ\n",
    "（**4.2.5 Negative Sampling**と**4.2.6 Negative Samplingのサンプリング手法**の内容を含みます）\n",
    "\n",
    "- 多値分類（これまでの方法）\n",
    " - you (　) hello ...\n",
    " - you と hello からターゲットを（$N$個の単語の中から）予測\n",
    "- **２値分類（高速化）**\n",
    " - [図４](https://www.dropbox.com/s/wrd9hpfonwi1opd/fig4.jpg?dl=0)\n",
    " - you (　) hello ...\n",
    " - you と hello からターゲットが **\"say\" であるかどうか**を予測（２値）\n",
    " - この場合，\"say\"の教師データを**正例**と呼ぶ．\n",
    " - \"say\"以外の単語が入力したとき，出力がどうなるかは何も言えない\n",
    "  - **Negative sampling （負例サンプリング）**\n",
    "- 負例サンプリング\n",
    " - すべての負例（$N-1$個の単語）を用いて学習するのではなく，$N-1$個の中からいくつかをランダムサンプリングして学習する\n",
    " - コーパスの単語の出現頻度（出現確率）に従ってランダムサンプリングを行う（一様乱数ではなく）\n",
    " - よく使われる単語はより正確に分散表現したいため．\n",
    " - ただし，ほとんど使われない単語も学習に使用される確率を（少し）高めるため，確率分布に以下の変換を施す．\n",
    " $$ P'(w_i) = \\frac{P(w_i)^{0.75}}{\\sum_{j=1}^N P(w_j)^{0.75}} $$\n",
    " - [図５](https://www.dropbox.com/s/pezn20yevsfkxj6/fig5.jpg?dl=0)\n",
    "\n",
    "### 4.2.3 シグモイド関数と交差エントロピー誤差\n",
    "- ２値分類のNN\n",
    " - [図6](https://www.dropbox.com/s/bli1x3j17jtam9r/Fig6.jpg?dl=0)\n",
    " - **スコア**$s\\in\\mathbb{R}$\n",
    "$$\n",
    "s = h W_{\\rm out}^{(j)} = \\langle h^\\top, W_{\\rm out}^{(j)} \\rangle\n",
    "$$\n",
    "- ２値分類の活性化関数：**シグモイド関数**\n",
    " $$ y = \\frac{1}{1+\\exp(-x)} $$\n",
    " - スコア $s\\in\\mathbb{R}$ を $[0,1]$の範囲の数（確率）に変換\n",
    " - 逆伝播\n",
    " $$ \\begin{split}\n",
    " \\frac{dy}{dx} &= \\frac{e^{-x}}{(1+e^{-x})^2}\\\\\n",
    " &= \\frac{1+e^{-x}-1}{(1+e^{-x})^2}\\\\\n",
    " &= \\frac{1}{1+e^{-x}} - \\frac{1}{(1+e^{-x})^2}\\\\\n",
    " &= y-y^2\\\\\n",
    " &= y(1-y)\n",
    " \\end{split}\n",
    " $$\n",
    " - [図７](https://www.dropbox.com/s/9iyszgdap27z95e/fig7.jpg?dl=0)\n",
    "- シグモイド関数に対応した誤差関数：**交差エントロピー誤差**\n",
    "$$ L = - \\bigr(t\\log y + (1-t) \\log(1-y)\\bigl) $$\n",
    " - 逆伝播\n",
    " $$ \\frac{\\partial L}{\\partial y} = - \\left( \\frac{t}{y} - \\frac{1-t}{1-y} \\right) $$\n",
    "  - これより --> [Sigm] --> [CE] --> の逆伝播は\n",
    "  $$ \\begin{split} \\frac{\\partial L}{\\partial y} y(1-y) &= - \\left( \\frac{t}{y} - \\frac{1-t}{1-y} \\right) y(1-y)\\\\\n",
    "  &= -t(1-y) + y(1-t)\\\\\n",
    "  &= -t + ty + y - ty\\\\\n",
    "  &= y-t\n",
    "  \\end{split}\n",
    "  $$\n",
    " - 出力$y\\in[0,1]$と正解ラベル$t\\in\\{0,1\\}$との誤差を逆伝播させれば良い（**誤差逆伝播**）\n",
    "\n",
    "### 4.2.4 多値分類から二値分類へ（実装編）\n",
    "- [図８](https://www.dropbox.com/s/ysr58lu4fqopvz2/fig8.jpg?dl=0)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbH_VyjszYOo"
   },
   "source": [
    "## 4.3 改良版word2vecの学習\n",
    "- 学習データ：PTBコーパス\n",
    " - Penn Tree Bank または Proposition Tree Bank の略\n",
    " - ペンシルベニア大学 (University of Pennsylvania) の研究グループが開発\n",
    " - 学習には非常に時間がかかるため，学習済みパラメータを使用 (cbow_params.pkl)\n",
    "\n",
    "### 4.3.3 CVOWモデルの評価\n",
    "- 単語の分散表現 $\\Rightarrow$ コサイン類似度による類似単語の表示\n",
    "$$ {\\rm similarity}(x,y) = \\frac{\\langle x,y \\rangle}{\\|x\\|\\cdot\\|y\\|} = \\cos(\\theta)$$\n",
    " - $\\theta$ は $x$と$y$のなす角\n",
    " - $\\langle x,y \\rangle = \\|x\\|\\|y\\|\\cos\\theta$\n",
    "- 単語の類推問題（アナロジー問題）\n",
    " - \"king - man + woman = queen\"\n",
    "  - king:man = woman:queen\n",
    " - ベクトルの加算と減算が単語の類推として意味を持つ\n",
    " - [図９](https://www.dropbox.com/s/57fp5l8evkfaeym/fig9.jpg?dl=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "executionInfo": {
     "elapsed": 819,
     "status": "error",
     "timestamp": 1611568118499,
     "user": {
      "displayName": "Masaaki Nagahara",
      "photoUrl": "",
      "userId": "06881010112898958132"
     },
     "user_tz": -540
    },
    "id": "HtqueF3Fcqk9",
    "outputId": "c18b3207-da48-4abd-df76-417f585c70ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " we: 0.6103515625\n",
      " someone: 0.59130859375\n",
      " i: 0.55419921875\n",
      " something: 0.48974609375\n",
      " anyone: 0.47314453125\n",
      "\n",
      "[query] year\n",
      " month: 0.71875\n",
      " week: 0.65234375\n",
      " spring: 0.62744140625\n",
      " summer: 0.6259765625\n",
      " decade: 0.603515625\n",
      "\n",
      "[query] car\n",
      " luxury: 0.497314453125\n",
      " arabia: 0.47802734375\n",
      " auto: 0.47119140625\n",
      " disk-drive: 0.450927734375\n",
      " travel: 0.4091796875\n",
      "\n",
      "[query] toyota\n",
      " ford: 0.55078125\n",
      " instrumentation: 0.509765625\n",
      " mazda: 0.49365234375\n",
      " bethlehem: 0.47509765625\n",
      " nissan: 0.474853515625\n",
      "--------------------------------------------------\n",
      "\n",
      "[analogy] king:man = queen:?\n",
      " woman: 5.16015625\n",
      " veto: 4.9296875\n",
      " ounce: 4.69140625\n",
      " earthquake: 4.6328125\n",
      " successor: 4.609375\n",
      "\n",
      "[analogy] take:took = go:?\n",
      " went: 4.55078125\n",
      " points: 4.25\n",
      " began: 4.09375\n",
      " comes: 3.98046875\n",
      " oct.: 3.90625\n",
      "\n",
      "[analogy] car:cars = child:?\n",
      " children: 5.21875\n",
      " average: 4.7265625\n",
      " yield: 4.20703125\n",
      " cattle: 4.1875\n",
      " priced: 4.1796875\n",
      "\n",
      "[analogy] good:better = bad:?\n",
      " more: 6.6484375\n",
      " less: 6.0625\n",
      " rather: 5.21875\n",
      " slower: 4.734375\n",
      " greater: 4.671875\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import most_similar, analogy\n",
    "import pickle\n",
    "\n",
    "\n",
    "pkl_file = 'cbow_params.pkl'\n",
    "# pkl_file = 'skipgram_params.pkl'\n",
    "\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    word_vecs = params['word_vecs']\n",
    "    word_to_id = params['word_to_id']\n",
    "    id_to_word = params['id_to_word']\n",
    "\n",
    "# most similar task\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)\n",
    "\n",
    "# analogy task\n",
    "print('-'*50)\n",
    "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3UdUpwEEcqlC",
    "outputId": "cbd9ccf3-03d9-44ed-f1bb-923becff4d66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] word\n",
      " precise: 0.497314453125\n",
      " road: 0.496826171875\n",
      " woman: 0.49609375\n",
      " player: 0.493408203125\n",
      " comedy: 0.490966796875\n"
     ]
    }
   ],
   "source": [
    "most_similar('word', word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fXEoSzFscqlC",
    "outputId": "f37faf24-dd18-44b0-a06c-cce9434e3a38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] cat:animal = human:?\n",
      " tv: 4.0078125\n",
      " damage: 3.7890625\n",
      " alto: 3.63671875\n",
      " professor: 3.51171875\n",
      " ms.: 3.498046875\n"
     ]
    }
   ],
   "source": [
    "analogy('cat', 'animal', 'human',  word_to_id, id_to_word, word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7VoM6XZzDiI"
   },
   "source": [
    "## 4.4 word2vecに関する残りのテーマ\n",
    "### 4.4.1 word2vecを使ったアプリケーション\n",
    "- 学習済みword2vecのNN$\\Rightarrow$転移学習 (transfer learning)\n",
    " - テキスト分類\n",
    " - 文章クラスタリング\n",
    " - 品詞タグ付け\n",
    " - 感情分析\n",
    "- word2vec: 単語を固定長($M$)のベクトルに変換$\\Rightarrow$**文章**を固定長のベクトルに変換\n",
    " - Bag-of-Words: 単語の順序を考慮しない\n",
    " - RNN（リカレントNN）: 単語の順序を考慮する\n",
    " - 例えば，メールの文章から送信者の感情を推定し分類（クレーム対応）\n",
    "### 4.4.2 単語ベクトルの評価方法\n",
    "- 分散表現の性能評価（どのくらい正確か）\n",
    "- あらかじめ人間が与えた類似度の評価セットを使う\n",
    " - 例：「catとanimalの類似度は８」，「catとcarの類似度は２」など\n",
    " - word2vecの出力と比較し性能を評価\n",
    "- **Semantics:** 単語の意味を類推\n",
    " - king:queen = actor:actressなど\n",
    "- **Syntax:** 単語の形態情報（文法）を問う問題\n",
    " - bad:more = good:bestなど\n",
    "- 図4-23 のデータではトータルではskip-gramのほうがCBOWよりも性能が良い\n",
    " - ただし，Syntaxに関しては，CBOWのほうが良い性能を示している "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKcYxB9EcqlD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Chapter4資料.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
